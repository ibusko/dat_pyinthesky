{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft - JWST Data Analysis Use Case: Crowded Field PSF Photometry\n",
    "\n",
    "**Data**: NIRCam simulated images obtained using [MIRAGE](https://jwst-docs.stsci.edu/jwst-other-tools/mirage-data-simulator) and run through the [JWST pipeline](https://jwst-pipeline.readthedocs.io/en/latest/) of the Large Magellanic Cloud (LMC) Astrometric Calibration Field. Simulations is obtained using a 4-pt subpixel dither for three couples of wide filters: F070W, F115W, and F200W for the SW channel, and F277W, F356W, and F444W for the LW channel. We simulated only 1 NIRCam SW detector (i.e., \"NRCB1\"). \n",
    "\n",
    "For this example, we use Level-2 images (.cal, calibrated but not rectified) for two SW filters (i.e., F115W and F200W) and derive the photometry in each one of them. The images for the other filters are also available and can be used to test the notebook and/or different filters combination. \n",
    "\n",
    "The notebook is divided in two parts: in Part I we show how to create a PSF model and perform the PSF photometry, whereas in Part II, we show how to derive the final calibrated Color-Magnitude Diagram.\n",
    "\n",
    "PSF Photometry can be obtained using:\n",
    "\n",
    "* single PSF obtained using WebbPSF\n",
    "* grid of PSFs obtained using WebbPSF\n",
    "* single effective PSF (ePSF)\n",
    "\n",
    "### Work in Progress:\n",
    "\n",
    "* create a grid of ePSFs and perform data reduction  \n",
    "* use the grid of ePSFs to perturbate the grid of WebbPSF PSFs\n",
    "\n",
    "The notebook shows:\n",
    "\n",
    "* how to obtain the PSF model from WebbPSF (or build an ePSF model)\n",
    "* how to perform PSF photometry on the image\n",
    "* how to cross-match the catalogs of the different images\n",
    "* how to derive and apply photometric zeropoints\n",
    "\n",
    "Final plots show:\n",
    "\n",
    "* Instrumental Color-Magnitude Diagrams for the 4 images\n",
    "* Instrumental Color-Magnitude Diagrams and errors\n",
    "* Photometric Zeropoints \n",
    "* Calibrated Color-Magnitude Diagram (compared with Input Color-Magnitude Diagram)\n",
    "* Comparison between input and output photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "import glob as glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import jwst\n",
    "from jwst.datamodels import ImageModel\n",
    "\n",
    "from astropy import wcs\n",
    "from astropy import units as u\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import (ZScaleInterval, SqrtStretch, ImageNormalize)\n",
    "from astropy.visualization import simple_norm\n",
    "from astropy.nddata import Cutout2D, NDData\n",
    "from astropy.stats import gaussian_sigma_to_fwhm\n",
    "from astropy.table import Table, QTable\n",
    "from astropy.modeling.fitting import LevMarLSQFitter\n",
    "from astropy.wcs.utils import pixel_to_skycoord\n",
    "from astropy.coordinates import SkyCoord, match_coordinates_sky\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "\n",
    "from photutils import CircularAperture, EPSFBuilder, find_peaks, CircularAnnulus\n",
    "from photutils.detection import DAOStarFinder, IRAFStarFinder\n",
    "from photutils.psf import DAOGroup, IntegratedGaussianPRF, extract_stars, IterativelySubtractedPSFPhotometry\n",
    "from photutils.background import MMMBackground, MADStdBackgroundRMS\n",
    "from photutils.centroids import centroid_2dg\n",
    "from photutils import aperture_photometry\n",
    "\n",
    "from ipywidgets import interact\n",
    " \n",
    "import webbpsf\n",
    "from webbpsf.utils import to_griddedpsfmodel\n",
    "\n",
    "import pysynphot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import style, pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "plt.rcParams['image.origin'] = 'lower'\n",
    "plt.rcParams['axes.titlesize'] =  plt.rcParams['axes.labelsize'] = 30\n",
    "plt.rcParams['xtick.labelsize'] =  plt.rcParams['ytick.labelsize'] = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Data Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load the images and create some useful dictionaries\n",
    "\n",
    "We load all the images and we create a dictionary that contains all of them, divided by detectors and filters. This is useful to check which detectors and filters are available and to decide if we want to perform the photometry on all of them or only on a subset (for example, only on the SW filters). \n",
    "\n",
    "We also create a dictionary with some useful parameters for the analysis (**Note**: this dictionary will be updated once the values for zeropoints will be available for each detectors after commissioning). \n",
    "\n",
    "Hence, we have two dictionaries:\n",
    "\n",
    "* dictionary for the single Level-2 calibrated images\n",
    "* dictionary with some other useful parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for the single images\n",
    "\n",
    "dict_images = {'NRCA1': {}, 'NRCA2': {}, 'NRCA3': {}, 'NRCA4': {}, 'NRCA5': {},\n",
    "                         'NRCB1': {}, 'NRCB2': {}, 'NRCB3': {}, 'NRCB4': {}, 'NRCB5': {}}\n",
    "\n",
    "dict_filter_short = {}\n",
    "dict_filter_long = {}\n",
    "\n",
    "ff_short = []\n",
    "det_short = []\n",
    "det_long = []\n",
    "ff_long = []\n",
    "detlist_short = []\n",
    "detlist_long = []\n",
    "filtlist_short = []\n",
    "filtlist_long = []\n",
    "\n",
    "boxlink_images_lev2 = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/stellar_photometry/images_level2.tar.gz'\n",
    "boxfile_images_lev2 = './images_level2.tar.gz'\n",
    "urllib.request.urlretrieve(boxlink_images_lev2,boxfile_images_lev2)\n",
    "\n",
    "tar = tarfile.open(boxfile_images_lev2, 'r')\n",
    "tar.extractall()\n",
    "\n",
    "files_singles = sorted(glob.glob( \"*cal.fits\"))\n",
    "\n",
    "for file in files_singles:\n",
    "    \n",
    "    im = fits.open(file)\n",
    "    f = im[0].header['FILTER']\n",
    "    d = im[0].header['DETECTOR']\n",
    "    \n",
    "    \n",
    "    if d == 'NRCBLONG':\n",
    "        d = 'NRCB5'\n",
    "    elif d == 'NRCALONG':\n",
    "        d = 'NRCA5'\n",
    "    else:\n",
    "        d = d\n",
    "        \n",
    "    wv = np.float(f[1:3])\n",
    "    \n",
    "    if wv > 24:         \n",
    "        ff_long.append(f)\n",
    "        det_long.append(d)\n",
    "        \n",
    "    else:\n",
    "        ff_short.append(f)\n",
    "        det_short.append(d)    \n",
    "    \n",
    "    detlist_short = sorted(list(dict.fromkeys(det_short)))\n",
    "    detlist_long = sorted(list(dict.fromkeys(det_long)))\n",
    "    \n",
    "    unique_list_filters_short = []\n",
    "    unique_list_filters_long = []\n",
    "\n",
    "    for x in ff_short:\n",
    "            \n",
    "        if x not in unique_list_filters_short:\n",
    "            \n",
    "            dict_filter_short.setdefault(x,{})\n",
    "        \n",
    "    for x in ff_long:\n",
    "        if x not in unique_list_filters_long:\n",
    "            dict_filter_long.setdefault(x,{})   \n",
    "    \n",
    "    for d_s in detlist_short:\n",
    "        dict_images[d_s] = dict_filter_short\n",
    "               \n",
    "    for d_l in detlist_long:\n",
    "        dict_images[d_l] = dict_filter_long\n",
    "    \n",
    "    filtlist_short = sorted(list(dict.fromkeys(dict_filter_short)))\n",
    "    filtlist_long = sorted(list(dict.fromkeys(dict_filter_long)))\n",
    "    \n",
    "    if len(dict_images[d][f]) == 0:\n",
    "        dict_images[d][f] = {'images': [file]}\n",
    "    else:\n",
    "        dict_images[d][f]['images'].append(file)\n",
    "\n",
    "        \n",
    "print(\"Available Detectors for SW channel:\", detlist_short)\n",
    "print(\"Available Detectors for LW channel:\", detlist_long)\n",
    "print(\"Available SW Filters:\", filtlist_short)\n",
    "print(\"Available LW Filters:\", filtlist_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['F070W', 'F090W', 'F115W', 'F140M', 'F150W2', 'F150W', 'F162M', 'F164N', 'F182M', \n",
    "           'F187N', 'F200W', 'F210M', 'F212N', 'F250M', 'F277W', 'F300M', 'F322W2', 'F323N',\n",
    "           'F335M', 'F356W', 'F360M', 'F405N', 'F410M', 'F430M', 'F444W', 'F460M', 'F466N', 'F470N', 'F480M']\n",
    "\n",
    "psf_fwhm = [0.987, 1.103, 1.298, 1.553, 1.628, 1.770, 1.801, 1.494, 1.990, 2.060, 2.141, 2.304, 2.341, 1.340,\n",
    "           1.444, 1.585, 1.547, 1.711, 1.760, 1.830, 1.901, 2.165, 2.179, 2.300, 2.302, 2.459, 2.507, 2.535, 2.574]\n",
    "\n",
    "\n",
    "zp_modA = [25.7977, 25.9686, 25.8419, 24.8878, 27.0048,25.6536, 24.6957, 22.3073, 24.8258, 22.1775, 25.3677, 24.3296,\n",
    "           22.1036, 22.7850, 23.5964, 24.8239, 23.6452, 25.3648, 20.8604, 23.5873, 24.3778, 23.4778, 20.5588,\n",
    "           23.2749, 22.3584, 23.9731, 21.9502, 20.0428, 19.8869, 21.9002]\n",
    "\n",
    "zp_modB = [25.7568, 25.9771, 25.8041, 24.8738, 26.9821, 25.6279, 24.6767, 22.2903, 24.8042, 22.1499, 25.3391, 24.2909,\n",
    "           22.0574, 22.7596, 23.5011, 24.6792, 23.5769, 25.3455, 20.8631, 23.4885, 24.3883, 23.4555, 20.7007,\n",
    "           23.2763, 22.4677, 24.1562, 22.0422, 20.1430, 20.0173, 22.4086]\n",
    "\n",
    "dict_utils = {filters[i]:{'psf fwhm': psf_fwhm[i], 'VegaMAG zp modA': zp_modA[i], \n",
    "                          'VegaMAG zp modB': zp_modB[i]} for i in range(len(filters))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the detectors and/or filters for the analysis\n",
    "\n",
    "If we are interested only in some filters (and some detectors) in the analysis, as in this example, we can select the Level-2 calibrated images from the dictionary for those filters and analyze only those images.\n",
    "\n",
    "In this particular example, we analyze images for filters **F115W** and **F200W** for the detector **NRCB1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets_short = ['NRCB1'] # detector of interest in this example\n",
    "filts_short = ['F115W', 'F200W'] # filters of interest in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the images\n",
    "\n",
    "To check that our images do not present artifacts and can be used in the analysis, we display them using an interactive cursor that allows to shuffle through the different images for each filter.\n",
    "\n",
    "### Note for developers: \n",
    "\n",
    "this is only a sketch of what I would like to show (I am not very familiar with ipywidgets). Would it be possible to show both filters at the same time, in a 2 window panel as in the static plot below? Or even better, have a widget control that allows to select the filters available and then use interact to cycle through the images? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell for display images using ipywidgets\n",
    "\n",
    "def browse_images(images):\n",
    "    n = len(images)\n",
    "    \n",
    "    def view_image(image):\n",
    "        det = 'NRCB1'\n",
    "        filt = 'F115W'\n",
    "        im = fits.open(dict_images[det][filt]['images'][image])\n",
    "        \n",
    "        data_sb = im[1].data\n",
    "        norm = simple_norm(data_sb, 'sqrt', percent=99.)   \n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "        plt.title(filt)\n",
    "        plt.imshow(data_sb, norm=norm, cmap='Greys')        \n",
    "        plt.show()\n",
    "        \n",
    "    interact(view_image, image=(0,n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browse_images(dict_images['NRCB1']['F115W']['images'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note for developers: \n",
    "\n",
    "Cell below should be removed once we finalize the interactive one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,14))\n",
    "\n",
    "for det in dets_short:\n",
    "    for i,filt  in enumerate(filts_short):\n",
    "            \n",
    "            image = fits.open(dict_images[det][filt]['images'][0])\n",
    "            data_sb = image[1].data\n",
    " \n",
    "            ax = plt.subplot(1,len(filts_short),i+1)\n",
    "    \n",
    "            plt.xlabel(\"X [px]\")\n",
    "            plt.ylabel(\"Y [px]\")\n",
    "            plt.title(filt)\n",
    "            norm = simple_norm(data_sb, 'sqrt', percent = 99.)\n",
    "\n",
    "            ax.imshow(data_sb, norm = norm, cmap = 'Greys')\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the PSF models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Create the PSF model using WebbPSF\n",
    "\n",
    "We create a dictionary that contains the PSFs created using WebbPSF for the detectors and filters selected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_psfs_webbpsf = {}\n",
    "\n",
    "for det in dets_short:\n",
    "    dict_psfs_webbpsf.setdefault(det,{})\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        dict_psfs_webbpsf[det].setdefault(filt,{})\n",
    "        \n",
    "        dict_psfs_webbpsf[det][filt]['psf model grid'] = None\n",
    "        dict_psfs_webbpsf[det][filt]['psf model single'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below allows to create a single PSF or a grid of PSFs and allows to save the PSFs as fits files or stored in the psf dictionary. For the grid of PSFs, users can select the number of PSFs to be created. The PSFs can be created detector-sampled or oversampled (the oversample can be changed inside the function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_psf_model(fov=11, create_grid=False, num=9, save_psf=False, detsampled=False):\n",
    "    \n",
    "    nrc = webbpsf.NIRCam()\n",
    "    \n",
    "    nrc.detector = det \n",
    "    nrc.filter = filt\n",
    "    \n",
    "    src = webbpsf.specFromSpectralType('G5V', catalog='phoenix')\n",
    "    if detsampled == True:\n",
    "        print(\"Creating a detector sampled PSF\")\n",
    "        aa = 'detector sampled'\n",
    "        fov = 21\n",
    "    else:\n",
    "        print(\"Creating a oversampled PSF\")\n",
    "        aa = 'oversampled'\n",
    "        fov = fov\n",
    "    \n",
    "    print(\"Using a {field}\".format(field=fov),\"px fov\")\n",
    "    \n",
    "    if create_grid == True:\n",
    "        print(\"\")\n",
    "        print(\"Creating a grid of PSF for filter {filt} and detector {det}\".format(filt=filt, det=det))\n",
    "        print(\"\")\n",
    "        num = num\n",
    "        \n",
    "        if save_psf == True:\n",
    "            \n",
    "            outname = \"../psf_models/PSF_%s_samp4_G5V_fov%d_npsfs%d.fits\" %(filt,fov,num)\n",
    "            nrc.psf_grid(num_psfs = num, oversample = 4, source = src, all_detectors = False, fov_pixels = fov, \n",
    "                     save = True, outfile = outname, use_detsampled_psf = detsampled)\n",
    "        else:\n",
    "            grid_psf = nrc.psf_grid(num_psfs = num, oversample = 4, source = src, all_detectors = False, \n",
    "                                    fov_pixels = fov,  use_detsampled_psf = detsampled)\n",
    "            dict_psfs_webbpsf[det][filt]['psf model grid'] = grid_psf\n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(\"Creating a single PSF for filter {filt} and detector {det}\".format(filt=filt, det=det))\n",
    "        print(\"\")\n",
    "        num = 1\n",
    "        if save_psf == True:\n",
    "            outname = \"../psf_models/PSF_%s_samp4_G5V_fov%d_npsfs%d.fits\" %(filt,fov,num)\n",
    "            nrc.psf_grid(num_psfs = num,  oversample = 4, source = src, all_detectors = False, fov_pixels = fov,\n",
    "                        save = True, outfile = outname, use_detsampled_psf = detsampled)\n",
    "        else:\n",
    "            single_psf = nrc.psf_grid(num_psfs = num,  oversample = 4, source = src, all_detectors = False, \n",
    "                                      fov_pixels = fov, use_detsampled_psf = detsampled)\n",
    "            dict_psfs_webbpsf[det][filt]['psf model single'] = single_psf\n",
    "        \n",
    "    return        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for det in dets_short:\n",
    "    for filt in filts_short:\n",
    "        create_psf_model(fov=11, num=25, create_grid=False, save_psf=False, detsampled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the single PSF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,14))\n",
    "\n",
    "for det in dets_short:\n",
    "    for i,filt in enumerate(filts_short):\n",
    "        ax = plt.subplot(1,2,i+1)\n",
    "    \n",
    "        norm_epsf = simple_norm(dict_psfs_webbpsf[det][filt]['psf model single'].data[0], 'log', percent=99.)\n",
    "        ax.set_title(filt, fontsize = 40)\n",
    "        ax.imshow(dict_psfs_webbpsf[det][filt]['psf model single'].data[0], norm=norm_epsf)\n",
    "        ax.set_xlabel('X [px]', fontsize = 30)\n",
    "        ax.set_ylabel('Y [px]', fontsize = 30)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the PSFs grid\n",
    "\n",
    "We show for 1 filter (**F115W**) the grid of PSFs and the difference from the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webbpsf.gridded_library.display_psf_grid(dict_psfs_webbpsf[dets_short[0]][filts_short[0]]['psf model grid'], \n",
    "                                         zoom_in = False, figsize = (14,14))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Create the PSF model building an Effective PSF (ePSF)\n",
    "\n",
    "More information on the PhotUtils Effective PSF can be found [here](https://photutils.readthedocs.io/en/stable/epsf.html).\n",
    "\n",
    "* Select the stars from the images we want to use to build the ePSF. We use the [DAOStarFinder](https://photutils.readthedocs.io/en/stable/api/photutils.detection.DAOStarFinder.html) function to find bright stars in the images (setting a high detection threshold).\n",
    "* Build the effective PSF (excluding objects for which the bounding box exceed the detector edge) using [EPSBuilder](https://photutils.readthedocs.io/en/stable/api/photutils.psf.EPSFBuilder.html#photutils.psf.EPSFBuilder) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case, we create a dictionary that contains the ePSFs for the detectors and filters selected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_psfs_epsf = {}\n",
    "\n",
    "for det in dets_short:\n",
    "    dict_psfs_epsf.setdefault(det,{})\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        dict_psfs_epsf[det].setdefault(filt,{})\n",
    "\n",
    "        dict_psfs_epsf[det][filt]['table psf stars'] = {}\n",
    "        dict_psfs_epsf[det][filt]['epsf single'] = {}\n",
    "        dict_psfs_epsf[det][filt]['epsf grid'] = {}\n",
    "        \n",
    "        for i in np.arange(0,len(dict_images[det][filt]['images']),1):\n",
    "        \n",
    "            dict_psfs_epsf[det][filt]['table psf stars'][i+1] = None\n",
    "            dict_psfs_epsf[det][filt]['epsf single'][i+1] = None\n",
    "            dict_psfs_epsf[det][filt]['epsf grid'][i+1] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Finding Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "bkgrms = MADStdBackgroundRMS()\n",
    "mmm_bkg = MMMBackground()\n",
    "\n",
    "th = [700,500] # threshold level for the two filters (length must match number of filters analyzed)\n",
    "\n",
    "for det in dets_short:\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        for i in np.arange(0,len(dict_images[det][filt]['images']),1):\n",
    "        \n",
    "            image = fits.open(dict_images[det][filt]['images'][i])\n",
    "            data_sb = image[1].data\n",
    "            imh = image[1].header\n",
    "        \n",
    "            print(\"Finding PSF stars on image {number} of filter {f}, detector {d}\".format(number = i+1, f=filt, d=det))\n",
    "        \n",
    "            data = data_sb / imh['PHOTMJSR']\n",
    "            print(\"Conversion factor from {units} to cps for filter {f}:\".format(units=imh['BUNIT'],f=filt), imh['PHOTMJSR'])\n",
    "        \n",
    "            sigma_psf = dict_utils[filt]['psf fwhm']\n",
    "        \n",
    "            print(\"FWHM for the filter {f}:\".format(f=filt), sigma_psf, \"px\")\n",
    "\n",
    "            std = bkgrms(data)\n",
    "            bkg = mmm_bkg(data)\n",
    "            daofind = DAOStarFinder(threshold=th[j]*std + bkg, fwhm=sigma_psf,roundhi=1.0, roundlo=-1.0, \n",
    "                                sharplo=0.30, sharphi=1.40)\n",
    "\n",
    "            psf_stars = daofind(data)\n",
    "            dict_psfs_epsf[det][filt]['table psf stars'][i+1] = psf_stars\n",
    "    \n",
    "            print(\"Number of sources used to build ePSF for {f}:\".format(f=filt), len(psf_stars))\n",
    "    \n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(\"Elapsed Time for finding PSF stars:\", toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Build Effective PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "sizes = [11, 11] # size of the cutout (extract region) for each PSF star - must match number of filters analyzed\n",
    "oversample = 4\n",
    "\n",
    "for det in dets_short:\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        for i in np.arange(0,len(dict_images[det][filt]['images']),1):\n",
    "        \n",
    "            image = fits.open(dict_images[det][filt]['images'][i])\n",
    "            data_sb = image[1].data\n",
    "            imh = image[1].header\n",
    "        \n",
    "            data = data_sb / imh['PHOTMJSR']\n",
    "        \n",
    "            hsize = (sizes[j] - 1) / 2\n",
    "        \n",
    "            x = dict_psfs_epsf[det][filt]['table psf stars'][i+1]['xcentroid']\n",
    "            y = dict_psfs_epsf[det][filt]['table psf stars'][i+1]['ycentroid']\n",
    "            mask = ((x > hsize) & (x < (data.shape[1] - 1 - hsize)) & (y > hsize) & (y < (data.shape[0] -1 - hsize)))\n",
    "    \n",
    "            stars_tbl = Table()\n",
    "            stars_tbl['x'] = x[mask]\n",
    "            stars_tbl['y'] = y[mask]\n",
    "    \n",
    "            bkg = mmm_bkg(data)\n",
    "\n",
    "            data_bkgsub = data.copy()\n",
    "    \n",
    "            data_bkgsub -= bkg\n",
    "    \n",
    "            nddata = NDData(data = data_bkgsub)\n",
    "            stars = extract_stars(nddata, stars_tbl, size = sizes[j])\n",
    "    \n",
    "            print(\"Creating ePSF for image {number} of filter {f}, detector {d}\".format(number = i+1, f=filt, d=det))\n",
    "        \n",
    "            epsf_builder = EPSFBuilder(oversampling = oversample, maxiters = 3, progress_bar = False)\n",
    "        \n",
    "            epsf, fitted_stars = epsf_builder(stars)\n",
    "            dict_psfs_epsf[det][filt]['epsf single'][i+1] = epsf\n",
    "\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(\"Time to build the Effective PSF:\", toc - tic)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the ePSFs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,14))\n",
    "\n",
    "for det in dets_short:\n",
    "    \n",
    "    for i,filt in enumerate(filts_short):\n",
    "        ax = plt.subplot(1,2,i+1)\n",
    "    \n",
    "        norm_epsf = simple_norm(dict_psfs_epsf[det][filt]['epsf single'][i+1].data, 'log', percent=99.)\n",
    "        plt.title(filt)\n",
    "        ax.imshow(dict_psfs_epsf[det][filt]['epsf single'][i+1].data, norm=norm_epsf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in Progress - Build a grid of ePSFs\n",
    "\n",
    "Two functions:\n",
    "* count PSF stars in the grid \n",
    "* create a grid of ePSFs\n",
    "\n",
    "The purpose of the first function is to count how many good PSF stars are in each sub-region defined by the grid number N. The function starts from the number provided by the user and iterate until the minimum grid size 2x2. Depending on the number of PSF stars that the users want in each cell of the grid, they can choose the appropriate grid size or modify the threshold values for the stars detection, selected when creating the single ePSF (in the **Finding stars** cell above). The minimum number is set in the function to 40 stars but can be also changed depending on the field properties and users need.\n",
    "\n",
    "The second function creates a grid of ePSFs with EPSFBuilder. The function will return a a GriddedEPSFModel object containing a 3D array of N  ×  n  ×  n. The 3D array represents the N number of 2D n  ×  n ePSFs created. It should include a grid_xypos key which will state the position of the PSF on the detector for each of the PSFs. The order of the tuples in grid_xypos refers to the number the PSF is in the 3D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Counting \"PSF stars\" in each region of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_PSFstars_grid(grid_points = 5, size = 15, min_numpsf = 40):\n",
    "    \n",
    "    num_grid_calc = np.arange(2,grid_points+1,1)\n",
    "    num_grid_calc = num_grid_calc[::-1]\n",
    "    \n",
    "    for num in num_grid_calc:\n",
    "        print(\"Calculating the number of PSF stars in a %d x %d grid:\" %(num,num))\n",
    "        print(\"\")\n",
    "        \n",
    "        image = fits.open(dict_images[det][filt]['images'][i])\n",
    "        data_sb = image[1].data\n",
    "\n",
    "        points = np.int16((data_sb.shape[0] / num)/2)\n",
    "        x_center = np.arange(points, 2*points*(num), 2*points)\n",
    "        y_center = np.arange(points, 2*points*(num), 2*points)\n",
    "            \n",
    "        centers = np.array(np.meshgrid(x_center, y_center)).T.reshape(-1,2)\n",
    "            \n",
    "        for n,val in enumerate(centers):\n",
    "        \n",
    "            x = dict_psfs_epsf[det][filt]['table psf stars'][i+1]['xcentroid']\n",
    "            y = dict_psfs_epsf[det][filt]['table psf stars'][i+1]['ycentroid']\n",
    "            flux = dict_psfs_epsf[det][filt]['table psf stars'][i+1]['flux']\n",
    "                \n",
    "            half_size = (size - 1) / 2\n",
    "                \n",
    "            lim1 = val[0] - points + half_size\n",
    "            lim2 = val[0] + points - half_size\n",
    "            lim3 = val[1] - points + half_size\n",
    "            lim4 = val[1] + points - half_size\n",
    "            \n",
    "            test = (x > lim1) & (x < lim2) & (y > lim3) & (y < lim4)\n",
    "            \n",
    "            #if np.count_nonzero(test) < min_numpsf:\n",
    "                #raise ValueError(\"Not enough PSF stars in all the cells (> %d): Decrease your grid size or the minimum number of PSF stars in each cell or change parameters in the finder\" %(min_numpsf))\n",
    "            if np.count_nonzero(test) < min_numpsf:\n",
    "                print(\"Center Coordinates of grid cell %d are (%d,%d) --- Not enough PSF stars in the cell (number of PSF stars < %d)\" %(i+1,val[0],val[1],min_numpsf))\n",
    "                \n",
    "            else:\n",
    "                print(\"Center Coordinate of grid cell %d are (%d,%d) --- Number of PSF stars:\" %(n+1,val[0],val[1]), np.count_nonzero(test))                \n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for det in dets_short:\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        for i in np.arange(0,len(dict_images[det][filt]['images']),1):\n",
    "            \n",
    "            print(\"Analyzing image {number} of filter {f}, detector {d} \".format(number = i+1, f=filt, d=det))\n",
    "            print(\"\")\n",
    "            \n",
    "            count_PSFstars_grid(grid_points = 5, size = 15, min_numpsf = 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - Create a grid of ePSFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here goes the function that creates a grid of ePSFs that can be saved in the epsf dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - Use the grid of ePSFs to perturbate the grid of WebbPSF PSFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here goes the function that create a grid of PSF models obtained perturbating the grid of WebbPSF PSFs using the grid of ePSFs created above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform PSF photometry\n",
    "\n",
    "We perform the PSF photometry on the images. The output catalogs and the residual images can be saved as ouput and are stored by default in the dictionary created in the cell below. **Note**: when photometry is rerun the files are overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_phot = {}\n",
    "\n",
    "for det in dets_short:\n",
    "    dict_phot.setdefault(det,{})\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        dict_phot[det].setdefault(filt,{})\n",
    "        \n",
    "        dict_phot[det][filt]['residual images'] = {}\n",
    "        dict_phot[det][filt]['output photometry tables'] = {}\n",
    "        \n",
    "        for i in np.arange(0,len(dict_images[det][filt]['images']),1):\n",
    "        \n",
    "            dict_phot[det][filt]['residual images'][i+1] = None\n",
    "            dict_phot[det][filt]['output photometry tables'][i+1] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: since performing the PSF photometry on the images takes some time (for the 8 images in this example ~ 4 hours), to speed up the notebook, we use a high threshold in the finding algorithm (finding detection threshold ~ 2000). In the analysis below, we use the catalogs obtained with a detection threshold ~ 10, from a previous reduction run. To perform a meaningful data reduction, the users should modify the threshold accordingly. \n",
    "\n",
    "Here we use as PSF model the grid of WebbPSF PSFs, but the users can change the model and use the others available (i.e., single WebbPSF PSF, single ePSF) commenting the appropriate lines in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_tot = time.perf_counter()\n",
    "\n",
    "bkgrms = MADStdBackgroundRMS()\n",
    "mmm_bkg = MMMBackground()\n",
    "fitter = LevMarLSQFitter()\n",
    "\n",
    "ap_radius = [3.0, 3.5] # must match the number of filters analyzed\n",
    "\n",
    "for det in dets_short:\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        for i in np.arange(0,len(dict_images[det][filt]['images']),1):\n",
    "            \n",
    "            im = fits.open(dict_images[det][filt]['images'][i])\n",
    "            imh = im[1].header\n",
    "            data_sb = im[1].data\n",
    "            \n",
    "            d = im[0].header['DETECTOR']\n",
    "            prim_dith_pos = im[0].header['PATT_NUM']\n",
    "            prim_dith_num = im[0].header['NUMDTHPT']\n",
    "            subpx_dith_pos = im[0].header['SUBPXNUM']\n",
    "            subpx_dith_num = im[0].header['SUBPXPNS']\n",
    "\n",
    "            data = data_sb / imh['PHOTMJSR']\n",
    "            \n",
    "            print(\"Conversion factor from {units} to cps for filter {f}:\".format(units=imh['BUNIT'],f=filt), imh['PHOTMJSR'])\n",
    "            print(\"Applying conversion to the data\")\n",
    "            \n",
    "            sigma_psf = dict_utils[filt]['psf fwhm']\n",
    "            print(\"FWHM for the filter {f}:\".format(f=filt), sigma_psf)\n",
    "    \n",
    "            std = bkgrms(data)\n",
    "            bkg = mmm_bkg(data)\n",
    "    \n",
    "            daofind = DAOStarFinder(threshold=2000*std + bkg, fwhm=sigma_psf,roundhi=1.0, roundlo=-1.0,\n",
    "                                    sharplo=0.30, sharphi=1.40)\n",
    "    \n",
    "            daogroup = DAOGroup(5.0*sigma_psf)\n",
    "            \n",
    "            #WebbPSF PSFs grid:\n",
    "            \n",
    "            psf_model = dict_psfs_webbpsf[det][filt]['psf model grid'].copy()\n",
    "        \n",
    "            # single WebbPSF PSF:\n",
    "            # psf_model = dict_psfs_webbpsf[det][filt]['psf model single'].copy()\n",
    "            \n",
    "            # single ePSF:\n",
    "            # psf_model = dict_psfs_epsf[det][filt]['epsf single'][i+1].copy()\n",
    "            \n",
    "            print(\"Performing the photometry on image {number} of filter {f}, detector {d}\".format(number = i+1, f=filt, d=det))\n",
    "            \n",
    "            tic = time.perf_counter()\n",
    "    \n",
    "            phot = IterativelySubtractedPSFPhotometry(finder= daofind, group_maker=daogroup,\n",
    "                                                bkg_estimator=mmm_bkg, psf_model=psf_model,\n",
    "                                                fitter=LevMarLSQFitter(),\n",
    "                                                niters=2, fitshape=(11,11), \n",
    "                                              aperture_radius = ap_radius[j])\n",
    "            result = phot(data)\n",
    "    \n",
    "            toc = time.perf_counter()\n",
    "    \n",
    "            print(\"Time needed to perform photometry on image {number}:\".format(number = i+1), \"%.2f\" %((toc-tic)/3600), \"hours\")\n",
    "            print(\"Number of sources detected in image {number} for filter {f}:\".format(number=i+1, f=filt), len(result))\n",
    "        \n",
    "            residual_image = phot.get_residual_image()\n",
    "                            \n",
    "            dict_phot[det][filt]['residual images'][i+1] = residual_image\n",
    "            dict_phot[det][filt]['output photometry tables'][i+1] =  result\n",
    "            \n",
    "            # save the residual images as fits file:\n",
    "            \n",
    "            save_fig = False\n",
    "            \n",
    "            if save_fig == True:\n",
    "                hdu = fits.PrimaryHDU(residual_image)\n",
    "                hdul = fits.HDUList([hdu])\n",
    "                residual_outname = residual_%s_%s_webbPSF_gridPSF_level2_%dof%d_%dof%d.fits' %(d,filt, prim_dith_pos, prim_dith_num, subpx_dith_pos, subpx_dith_num)\n",
    "                \n",
    "                dir_output_photometry = './'\n",
    "                \n",
    "                hdul.writeto(os.path.join(dir_output_phot, residual_outname))\n",
    "            \n",
    "                outname = 'phot_%s_%s_webbPSF_gridPSF_level2_%dof%d_%dof%d.pkl' %(d,filt, prim_dith_pos, prim_dith_num, subpx_dith_pos, subpx_dith_num)\n",
    "    \n",
    "                # save the output photometry Tables\n",
    "    \n",
    "                tab = result.to_pandas()\n",
    "                tab.to_pickle(os.path.join(dir_output_phot, outname))\n",
    "            \n",
    "\n",
    "    \n",
    "toc_tot = time.perf_counter()\n",
    "print(\"Time elapsed to perform the photometry of the {number} images:\".format(number=(len(filts_short)*len(dict_images[det][filt]['images']))), \"%.2f\" %((toc_tot - tic_tot)/3600), \"hours\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note for developer: \n",
    "\n",
    "It would be really useful, if PhotUtils can provide some diagnostics to identify the quality of the photometry in the final catalog for each source (similarly to all the other PSF photometry programs available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: here we use the catalogs obtained using a grid of WebbPSF PSFs in the data reduction. The users can perform the data analysis using the catalogs obtained from the PSF photometry adopting a different PSF model (i.e., single PSF model, PSF grid, etc.) and compare the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tables with PSF Photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxlink_cat_f115w = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/stellar_photometry/phot_cat_F115W.tar.gz'\n",
    "boxfile_cat_f115w = './phot_cat_F115W.tar.gz'\n",
    "urllib.request.urlretrieve(boxlink_cat_f115w,boxfile_cat_f115w)\n",
    "\n",
    "tar = tarfile.open(boxfile_cat_f115w, 'r')\n",
    "tar.extractall()\n",
    "\n",
    "boxlink_cat_f200w = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/stellar_photometry/phot_cat_F200W.tar.gz'\n",
    "boxfile_cat_f200w = './phot_cat_F200W.tar.gz'\n",
    "urllib.request.urlretrieve(boxlink_cat_f200w,boxfile_cat_f200w)\n",
    "\n",
    "tar = tarfile.open(boxfile_cat_f200w, 'r')\n",
    "tar.extractall()\n",
    "\n",
    "\n",
    "phots_pkl_f115w = sorted(glob.glob('*F115W*gridPSF*'))\n",
    "phots_pkl_f200w = sorted(glob.glob('*F200W*gridPSF*'))                         \n",
    "\n",
    "results_f115w = []\n",
    "results_f200w = []\n",
    "\n",
    "for phot_pkl_f115w,phot_pkl_f200w in zip(phots_pkl_f115w,phots_pkl_f200w):\n",
    "    ph_f115w = pd.read_pickle(phot_pkl_f115w)\n",
    "    ph_f200w = pd.read_pickle(phot_pkl_f200w)\n",
    "    \n",
    "    result_f115w = QTable.from_pandas(ph_f115w)\n",
    "    result_f200w = QTable.from_pandas(ph_f200w)\n",
    "    \n",
    "    results_f115w.append(result_f115w)\n",
    "    results_f200w.append(result_f200w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the images to DataModel\n",
    "\n",
    "In order to assign the WCS coordinate and hence cross-match the images, we need to transfrom the images to DataModel. The coordinates are assigned during the step [assign_wcs](https://jwst-pipeline.readthedocs.io/en/stable/jwst/assign_wcs/main.html?#using-the-wcs-interactively) step in the JWST pipeline and allow us to cross-match the different catalogs obtained for each filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_f115w = []\n",
    "images_f200w = []\n",
    "\n",
    "for i in np.arange(0,len(dict_images['NRCB1']['F115W']['images']),1):\n",
    "        image_f115w = ImageModel(dict_images['NRCB1']['F115W']['images'][i])\n",
    "        images_f115w.append(image_f115w)\n",
    "        \n",
    "for i in np.arange(0,len(dict_images['NRCB1']['F200W']['images']),1):\n",
    "        image_f200w = ImageModel(dict_images['NRCB1']['F200W']['images'][i])\n",
    "        images_f200w.append(image_f200w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-match the catalogs from the two filters for the 4 images\n",
    "\n",
    "We cross-match the catalogs to obtain the single color-magnitude diagrams.\n",
    "\n",
    "Stars from the two filters are associated if the distance between the matches is < 0.5 px. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clean_f115w = []\n",
    "results_clean_f200w = []\n",
    "\n",
    "for i in np.arange(0,len(images_f115w),1):\n",
    "    \n",
    "    mask_f115w = ((results_f115w[i]['x_fit'] > 0) & (results_f115w[i]['x_fit'] < 2048) & \n",
    "               (results_f115w[i]['y_fit'] > 0) & (results_f115w[i]['y_fit'] < 2048) &\n",
    "               (results_f115w[i]['flux_fit'] > 0))\n",
    "    \n",
    "    result_clean_f115w = results_f115w[i][mask_f115w]\n",
    "    \n",
    "    ra_f115w,dec_f115w = images_f115w[i].meta.wcs(result_clean_f115w['x_fit'],result_clean_f115w['y_fit'])\n",
    "    radec_f115w = SkyCoord(ra_f115w,dec_f115w, unit = 'deg')\n",
    "    result_clean_f115w['radec'] = radec_f115w\n",
    "    results_clean_f115w.append(result_clean_f115w)\n",
    "\n",
    "    mask_f200w = ((results_f200w[i]['x_fit'] > 0) & (results_f200w[i]['x_fit'] < 2048) & \n",
    "               (results_f200w[i]['y_fit'] > 0) & (results_f200w[i]['y_fit'] < 2048) &\n",
    "               (results_f200w[i]['flux_fit'] > 0))\n",
    "    \n",
    "    result_clean_f200w = results_f200w[i][mask_f200w]\n",
    "    \n",
    "    ra_f200w,dec_f200w = images_f200w[i].meta.wcs(result_clean_f200w['x_fit'],result_clean_f200w['y_fit'])\n",
    "    radec_f200w = SkyCoord(ra_f200w,dec_f200w, unit = 'deg')\n",
    "\n",
    "    result_clean_f200w['radec'] = radec_f200w\n",
    "    results_clean_f200w.append(result_clean_f200w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sep = 0.015 * u.arcsec\n",
    "\n",
    "matches_phot_single = []\n",
    "\n",
    "for res1,res2 in zip(results_clean_f115w,results_clean_f200w):\n",
    "    \n",
    "    idx, d2d, _ = match_coordinates_sky(res1['radec'], res2['radec'])\n",
    "\n",
    "    sep_constraint = d2d < max_sep\n",
    "    \n",
    "    match_phot_single = Table()\n",
    "    \n",
    "    x_f115w = res1['x_fit'][sep_constraint]\n",
    "    y_f115w = res1['y_fit'][sep_constraint]\n",
    "    radec_f115w = res1['radec'][sep_constraint]\n",
    "    mag_f115w = (-2.5 * np.log10(res1['flux_fit']))[sep_constraint]\n",
    "    emag_f115w = (1.086 * (res1['flux_unc'] / res1['flux_fit']))[sep_constraint]\n",
    "    \n",
    "    x_f200w = res2['x_fit'][idx[sep_constraint]]\n",
    "    y_f200w = res2['y_fit'][idx[sep_constraint]]\n",
    "    radec_f200w = res2['radec'][idx][sep_constraint]\n",
    "    mag_f200w = (-2.5 * np.log10(res2['flux_fit']))[idx[sep_constraint]]\n",
    "    emag_f200w = (1.086 * (res2['flux_unc'] / res2['flux_fit']))[idx[sep_constraint]]\n",
    "    \n",
    "    match_phot_single['x_f115w'] = x_f115w\n",
    "    match_phot_single['y_f115w'] = y_f115w\n",
    "    match_phot_single['radec_f115w'] = radec_f115w\n",
    "    match_phot_single['mag_f115w'] = mag_f115w\n",
    "    match_phot_single['emag_f115w'] = emag_f115w\n",
    "    match_phot_single['x_f200w'] = x_f200w\n",
    "    match_phot_single['y_f200w'] = y_f200w\n",
    "    match_phot_single['radec_f200w'] = radec_f200w\n",
    "    match_phot_single['mag_f200w'] = mag_f200w\n",
    "    match_phot_single['emag_f200w'] = emag_f200w\n",
    "    \n",
    "    matches_phot_single.append(match_phot_single)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color-Magnitude Diagrams (Instrumental Magnitudes) for the 4 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,16))\n",
    "plt.clf()\n",
    "\n",
    "font1 = {'family' : 'serif',\n",
    "      'color': 'black',\n",
    "      'weight': 'normal',\n",
    "      'size': '12'}\n",
    "\n",
    "font2 = {'family' : 'serif',\n",
    "      'color': 'black',\n",
    "      'weight': 'normal',\n",
    "      'size': '20'}\n",
    "\n",
    "filt1 = 'F115W'\n",
    "filt2 = 'F200W'\n",
    "\n",
    "for i in np.arange(0,len(matches_phot_single),1):\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "   \n",
    "    j = str(i+1)\n",
    "    \n",
    "    xlim0 = -0.5\n",
    "    xlim1 = 0.8\n",
    "    ylim0 = -1 \n",
    "    ylim1 = -9\n",
    "\n",
    "\n",
    "    ax.set_xlim(xlim0,xlim1)\n",
    "    ax.set_ylim(ylim0,ylim1)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "    ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "    ax.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "    \n",
    "    f115w_single = matches_phot_single[i]['mag_f115w']\n",
    "    f200w_single = matches_phot_single[i]['mag_f200w']\n",
    "    \n",
    "    ax.scatter(f115w_single - f200w_single, f115w_single, s = 1, color = 'k')\n",
    "\n",
    "    ax.set_xlabel(filt1+'_inst - '+filt2+'_inst', fontdict = font2)\n",
    "    ax.set_ylabel(filt1+'_inst', fontdict = font2)\n",
    "    ax.text(xlim0+0.1, -8.65, \"Image %s\" %j, fontdict = font2)\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-match the 4 catalogs for each filter\n",
    "\n",
    "To obtain a final color-magnitude diagram, we need to cross-match all the catalogs for each filters and then cross-match the derived final catalogs.\n",
    "\n",
    "**Note**: this is the most conservative approach since we impose that a star must be found in all 4 catalogs.\n",
    "\n",
    "### Note for developer: \n",
    "\n",
    "I couldn't find an easier way to write this function, where you need to match the first two catalogs, derive a sub-catalogs with only the matches and then iterate for all the other catalogs available for a specific filter. We should also think on how to create a function that allows to keep the stars also if they are available in X out of Y catalogs (i.e., if for some reasons, a measure is not available in 1 of the images, but the star is well measured in the other 3, it doesn't make sense to discard the object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossmatch_filter(table=None):\n",
    "    num = 0\n",
    "    num_cat = np.char.mod('%d', np.arange(1,len(table)+1,1))\n",
    "\n",
    "    idx_12, d2d_12,_ = match_coordinates_sky(table[num]['radec'], table[num+1]['radec'])\n",
    "\n",
    "    sep_constraint_12 = d2d_12 < max_sep\n",
    "\n",
    "    matches_12 = Table()\n",
    "\n",
    "    matches_12['radec'] = table[num]['radec'][sep_constraint_12]\n",
    "    matches_12['mag_'+num_cat[num]] =  (-2.5 * np.log10(table[num]['flux_fit']))[sep_constraint_12]\n",
    "    matches_12['emag_'+num_cat[num]] = (1.086 * (table[num]['flux_unc'] / \n",
    "                                     table[num]['flux_fit']))[sep_constraint_12]\n",
    "\n",
    "    matches_12['mag_'+num_cat[num+1]] = (-2.5 * np.log10(table[num+1]['flux_fit']))[idx_12[sep_constraint_12]]\n",
    "    matches_12['emag_'+num_cat[num+1]] = (1.086 * (table[num+1]['flux_unc'] / \n",
    "                                     table[num+1]['flux_fit']))[idx_12[sep_constraint_12]]\n",
    "\n",
    "    idx_123, d2d_123,_ = match_coordinates_sky(matches_12['radec'], table[num+2]['radec'])\n",
    "\n",
    "    sep_constraint_123 = d2d_123 < max_sep\n",
    "\n",
    "    matches_123 = Table()\n",
    "\n",
    "    matches_123['radec'] = matches_12['radec'][sep_constraint_123]\n",
    "    matches_123['mag_'+num_cat[num]] = matches_12['mag_'+num_cat[num]][sep_constraint_123]\n",
    "    matches_123['emag_'+num_cat[num]] = matches_12['emag_'+num_cat[num]][sep_constraint_123]\n",
    "    matches_123['mag_'+num_cat[num+1]] = matches_12['mag_'+num_cat[num+1]][sep_constraint_123]\n",
    "    matches_123['emag_'+num_cat[num+1]] = matches_12['emag_'+num_cat[num+1]][sep_constraint_123]\n",
    "    matches_123['mag_'+num_cat[num+2]] = (-2.5 * np.log10(table[num+2]['flux_fit']))[idx_123[sep_constraint_123]]\n",
    "    matches_123['emag_'+num_cat[num+2]] = (1.086 * (table[num+2]['flux_unc'] / \n",
    "                                     table[num+2]['flux_fit']))[idx_123[sep_constraint_123]]\n",
    "\n",
    "    idx_1234, d2d_1234,_ = match_coordinates_sky(matches_123['radec'], table[num+3]['radec'])\n",
    "\n",
    "    sep_constraint_1234 = d2d_1234 < max_sep\n",
    "\n",
    "    matches_1234 = Table()\n",
    "\n",
    "    matches_1234['radec'] = matches_123['radec'][sep_constraint_1234]\n",
    "    matches_1234['mag_'+num_cat[num]] = matches_123['mag_'+num_cat[num]][sep_constraint_1234]\n",
    "    matches_1234['emag_'+num_cat[num]] = matches_123['emag_'+num_cat[num]][sep_constraint_1234]\n",
    "    matches_1234['mag_'+num_cat[num+1]] = matches_123['mag_'+num_cat[num+1]][sep_constraint_1234]\n",
    "    matches_1234['emag_'+num_cat[num+1]] = matches_123['emag_'+num_cat[num+1]][sep_constraint_1234]\n",
    "    matches_1234['mag_'+num_cat[num+2]] = matches_123['mag_'+num_cat[num+2]][sep_constraint_1234]\n",
    "    matches_1234['emag_'+num_cat[num+2]] = matches_123['emag_'+num_cat[num+2]][sep_constraint_1234]\n",
    "    matches_1234['mag_'+num_cat[num+3]] = (-2.5 * np.log10(table[num+3]['flux_fit']))[idx_1234[sep_constraint_1234]]\n",
    "    matches_1234['emag_'+num_cat[num+3]] = (1.086 * (table[num+3]['flux_unc'] / \n",
    "                                     table[num+3]['flux_fit']))[idx_1234[sep_constraint_1234]]\n",
    "\n",
    "    matches_1234\n",
    "     \n",
    "    return matches_1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_f115w = crossmatch_filter(table=results_clean_f115w)\n",
    "matches_f200w = crossmatch_filter(table=results_clean_f200w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final catalog, we assume that the magnitude is the mean of the 4 measures and the error on the magnitude is its standard deviation.\n",
    "\n",
    "To easily perform this arithmetic operation on the table, we convert the table to pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f115w = matches_f115w.to_pandas()\n",
    "df_f200w = matches_f200w.to_pandas()\n",
    "\n",
    "df_f115w['f115w_inst'] = df_f115w[['mag_1', 'mag_2', 'mag_3', 'mag_4']].mean(axis=1)\n",
    "df_f115w['ef115w_inst'] = df_f115w[['mag_1', 'mag_2', 'mag_3', 'mag_4']].std(axis=1)\n",
    "df_f200w['f200w_inst'] = df_f200w[['mag_1', 'mag_2', 'mag_3', 'mag_4']].mean(axis=1)\n",
    "df_f200w['ef200w_inst'] = df_f200w[['mag_1', 'mag_2', 'mag_3', 'mag_4']].std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Color-Magnitude Diagram (Instrumental Magnitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,14))\n",
    "plt.clf()\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "\n",
    "ax1.set_xlabel(filt1+'_inst - '+filt2+'_inst', fontdict = font2)\n",
    "ax1.set_ylabel(filt1+'_inst', fontdict = font2)\n",
    "\n",
    "xlim0 = -0.5\n",
    "xlim1 = 0.8\n",
    "ylim0 = -1.5\n",
    "ylim1 = -9\n",
    "\n",
    "ax1.set_xlim(xlim0,xlim1)\n",
    "ax1.set_ylim(ylim0,ylim1)\n",
    "\n",
    "ax1.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax1.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "radec_f115w_inst = SkyCoord(df_f115w['radec.ra'],df_f115w['radec.dec'], unit = 'deg')\n",
    "radec_f200w_inst = SkyCoord(df_f200w['radec.ra'],df_f200w['radec.dec'], unit = 'deg')\n",
    "\n",
    "idx_inst, d2d_inst, _ = match_coordinates_sky(radec_f115w_inst,radec_f200w_inst)\n",
    "\n",
    "sep_constraint_inst = d2d_inst < max_sep\n",
    "\n",
    "f115w_inst = np.array(df_f115w['f115w_inst'][sep_constraint_inst])\n",
    "ef115w_inst = np.array(df_f115w['ef115w_inst'][sep_constraint_inst])\n",
    "radec_f115w = radec_f115w_inst[sep_constraint_inst]\n",
    "\n",
    "f200w_inst = np.array(df_f200w['f200w_inst'][idx_inst[sep_constraint_inst]])\n",
    "ef200w_inst = np.array(df_f200w['ef200w_inst'][idx_inst[sep_constraint_inst]])\n",
    "radec_f200w = radec_f200w_inst[idx_inst[sep_constraint_inst]]\n",
    "\n",
    "ax1.scatter(f115w_inst - f200w_inst, f115w_inst,  s = 1, color = 'k')\n",
    "\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "\n",
    "ax2.set_xlabel(filt1+'_inst', fontdict = font2)\n",
    "ax2.set_ylabel('$\\sigma$'+filt1, fontdict = font2)\n",
    "\n",
    "xlim0 = -9\n",
    "xlim1 = -1.5\n",
    "ylim0 = -0.01 \n",
    "ylim1 = 1\n",
    "\n",
    "ax2.set_xlim(xlim0,xlim1)\n",
    "ax2.set_ylim(ylim0,ylim1)\n",
    "\n",
    "ax2.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax2.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax2.scatter(df_f115w['f115w_inst'],df_f115w['ef115w_inst'], s = 1, color = 'k')\n",
    "\n",
    "ax3 = plt.subplot(2,2,4)\n",
    "\n",
    "ax3.set_xlabel(filt2+'_inst', fontdict = font2)\n",
    "ax3.set_ylabel('$\\sigma$'+filt2, fontdict = font2)\n",
    "\n",
    "ax3.set_xlim(xlim0,xlim1)\n",
    "ax3.set_ylim(ylim0,ylim1)\n",
    "\n",
    "ax3.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax3.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax3.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax3.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax3.scatter(df_f200w['f200w_inst'],df_f200w['ef200w_inst'], s = 1, color = 'k')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photometric Zeropoints\n",
    "\n",
    "To obtain the final calibrated color-magnitude diagram, we need to calculate the photometric zeropoints. Hence we need to perform aperture photometry on the calibrated and rectified images (Level-3), apply the appropriate aperture correction for the finite aperture adopted (the values provided in the dictionary above are for an infinite aperture) and then compare it with the PSF photometry. Hence, we can summarize the steps as follows:\n",
    "\n",
    "* perform aperture photometry \n",
    "* apply appropriate aperture correction\n",
    "* apply tabulated zeropoint\n",
    "* cross-match with psf photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the calibrated and rectified images (Level-3 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_images_combined = {'NRCA1': {}, 'NRCA2': {}, 'NRCA3': {}, 'NRCA4': {}, 'NRCA5': {},\n",
    "                         'NRCB1': {}, 'NRCB2': {}, 'NRCB3': {}, 'NRCB4': {}, 'NRCB5': {}}\n",
    "\n",
    "dict_filter_short = {}\n",
    "dict_filter_long = {}\n",
    "\n",
    "ff_short = []\n",
    "det_short = []\n",
    "det_long = []\n",
    "ff_long = []\n",
    "detlist_short = []\n",
    "detlist_long = []\n",
    "filtlist_short = []\n",
    "filtlist_long = []\n",
    "\n",
    "boxlink_images_lev3 = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/stellar_photometry/images_level3.tar.gz'\n",
    "boxfile_images_lev3 = './images_level3.tar.gz'\n",
    "urllib.request.urlretrieve(boxlink_images_lev3,boxfile_images_lev3)\n",
    "\n",
    "tar = tarfile.open(boxfile_images_lev3, 'r')\n",
    "tar.extractall()\n",
    "\n",
    "\n",
    "files_singles = sorted(glob.glob( \"*combined*\"))\n",
    "\n",
    "for file in files_singles:\n",
    "    \n",
    "    im = fits.open(file)\n",
    "    f = im[0].header['FILTER']\n",
    "    d = im[0].header['DETECTOR']\n",
    "    \n",
    "    \n",
    "    if d == 'NRCBLONG':\n",
    "        d = 'NRCB5'\n",
    "    elif d == 'NRCALONG':\n",
    "        d = 'NRCA5'\n",
    "    else:\n",
    "        d = d\n",
    "        \n",
    "    wv = np.float(f[1:3])\n",
    "    \n",
    "    if wv > 24:         \n",
    "        ff_long.append(f)\n",
    "        det_long.append(d)\n",
    "        \n",
    "    else:\n",
    "        ff_short.append(f)\n",
    "        det_short.append(d)    \n",
    "    \n",
    "    detlist_short = sorted(list(dict.fromkeys(det_short)))\n",
    "    detlist_long = sorted(list(dict.fromkeys(det_long)))\n",
    "    \n",
    "    unique_list_filters_short = []\n",
    "    unique_list_filters_long = []\n",
    "\n",
    "    for x in ff_short:\n",
    "            \n",
    "        if x not in unique_list_filters_short:\n",
    "            \n",
    "            dict_filter_short.setdefault(x,{})\n",
    "        \n",
    "    for x in ff_long:\n",
    "        if x not in unique_list_filters_long:\n",
    "            dict_filter_long.setdefault(x,{})   \n",
    "    \n",
    "    for d_s in detlist_short:\n",
    "        dict_images_combined[d_s] = dict_filter_short\n",
    "               \n",
    "    for d_l in detlist_long:\n",
    "        dict_images_combined[d_l] = dict_filter_long\n",
    "    \n",
    "    filtlist_short = sorted(list(dict.fromkeys(dict_filter_short)))\n",
    "    filtlist_long = sorted(list(dict.fromkeys(dict_filter_long)))\n",
    "    \n",
    "    if len(dict_images_combined[d][f]) == 0:\n",
    "        dict_images_combined[d][f] = {'images': [file]}\n",
    "    else:\n",
    "        dict_images_combined[d][f]['images'].append(file)\n",
    "\n",
    "        \n",
    "print(\"Available Detectors for SW channel:\", detlist_short)\n",
    "print(\"Available Detectors for LW channel:\", detlist_long)\n",
    "print(\"Available SW Filters:\", filtlist_short)\n",
    "print(\"Available LW Filters:\", filtlist_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Display the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,14))\n",
    "\n",
    "for det in dets_short:\n",
    "    for i,filt  in enumerate(filts_short):\n",
    "        \n",
    "            image = fits.open(dict_images_combined[det][filt]['images'][0])\n",
    "            data_sb = image[1].data\n",
    "    \n",
    "            ax = plt.subplot(1,len(filts_short),i+1)\n",
    "    \n",
    "            norm = simple_norm(data_sb, 'sqrt', percent = 99.)\n",
    "            plt.xlabel(\"X [px]\")\n",
    "            plt.ylabel(\"Y [px]\")\n",
    "            plt.title(filt)\n",
    "    \n",
    "            ax.imshow(data_sb, norm = norm, cmap = 'Greys')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperture Photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done previously, we create a dictionary that contains the tables with the found stars and the tables with the derived aperture photometry for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_aper = {}\n",
    "\n",
    "for det in dets_short:\n",
    "    dict_aper.setdefault(det,{})\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        dict_aper[det].setdefault(filt,{})\n",
    "        \n",
    "        dict_aper[det][filt]['stars for ap phot'] = None\n",
    "        dict_aper[det][filt]['stars for ap phot matched'] = None\n",
    "        dict_aper[det][filt]['aperture phot table'] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find bright isolated stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "bkgrms = MADStdBackgroundRMS()\n",
    "mmm_bkg = MMMBackground()\n",
    "\n",
    "th = [700,500] # threshold level for the two filters (length must match number of filters analyzed)\n",
    "\n",
    "for det in dets_short:\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        for i in np.arange(0,len(dict_images_combined[det][filt]['images']),1):\n",
    "        \n",
    "            image = fits.open(dict_images_combined[det][filt]['images'][i])\n",
    "            data_sb = image[1].data\n",
    "            imh = image[1].header\n",
    "        \n",
    "            print(\"Selecting stars for aperture photometry on image {number} of filter {f}, detector {d}\".format(number = i+1, f=filt, d=det))\n",
    "        \n",
    "            data = data_sb / imh['PHOTMJSR']\n",
    "            print(\"Conversion factor from {units} to cps for filter {f}:\".format(units=imh['BUNIT'],f=filt), imh['PHOTMJSR'])\n",
    "        \n",
    "            sigma_psf = dict_utils[filt]['psf fwhm']\n",
    "        \n",
    "            print(\"FWHM for the filter {f}:\".format(f=filt), sigma_psf, \"px\")\n",
    "\n",
    "            std = bkgrms(data)\n",
    "            bkg = mmm_bkg(data)\n",
    "            daofind = DAOStarFinder(threshold=th[j]*std + bkg, fwhm=sigma_psf,roundhi=1.0, roundlo=-1.0, \n",
    "                                sharplo=0.30, sharphi=1.40)\n",
    "\n",
    "            apcorr_stars = daofind(data)\n",
    "            dict_aper[det][filt]['stars for ap phot'] = apcorr_stars\n",
    "    \n",
    "            print(\"Number of sources for {f} filter:\".format(f=filt), len(apcorr_stars))\n",
    "    \n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(\"Elapsed Time for finding stars for Aperture Photometry:\", toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a further way to obtain a good quality sample, we cross-match the catalogs from the two filters and retain only the stars in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for det in dets_short:\n",
    "    for j,filt in enumerate(filts_short):\n",
    "        for i in np.arange(0,len(dict_images_combined[det][filt]['images']),1):\n",
    "\n",
    "            image = ImageModel(dict_images_combined[det][filt]['images'][i])\n",
    "\n",
    "            ra,dec = image.meta.wcs(dict_aper[det][filt]['stars for ap phot']['xcentroid'],\n",
    "                                              dict_aper[det][filt]['stars for ap phot']['ycentroid'])\n",
    "        \n",
    "            radec = SkyCoord(ra,dec, unit = 'deg')\n",
    "            dict_aper[det][filt]['stars for ap phot']['radec'] = radec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1 = 'F115W'\n",
    "filt2 = 'F200W'\n",
    "\n",
    "idx_ap, d2d_ap, _ = match_coordinates_sky(dict_aper[det][filt1]['stars for ap phot']['radec'],\n",
    "                                              dict_aper[det][filt2]['stars for ap phot']['radec'])\n",
    "\n",
    "sep_constraint_ap = d2d_ap < max_sep\n",
    "\n",
    "matched_apcorr_f115w = Table()\n",
    "matched_apcorr_f200w = Table()\n",
    "\n",
    "matched_apcorr_f115w = dict_aper[det][filt1]['stars for ap phot'][sep_constraint_ap]\n",
    "matched_apcorr_f200w = dict_aper[det][filt2]['stars for ap phot'][idx_ap[sep_constraint_ap]]\n",
    "\n",
    "dict_aper[det][filt1]['stars for ap phot matched'] = matched_apcorr_f115w\n",
    "dict_aper[det][filt2]['stars for ap phot matched'] = matched_apcorr_f200w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load aperture correction table\n",
    "\n",
    "**Note**: these values are obtained from the study of the synthetic WebbPSF PSFs. They will be updated once we have in-flight measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ap_tab = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/stellar_photometry/aperture_correction_table.txt'\n",
    "\n",
    "aper_table = pd.read_csv(ap_tab, header = None, sep='\\s+', index_col = 0,\n",
    "                         names=['filter', 'pupil', 'wave','r10','r20', 'r30', 'r40', \n",
    "                                                                      'r50','r60','r70','r80','r85','r90', \n",
    "                                                                      'sky_flux_px','apcorr10','apcorr20','apcorr30',\n",
    "                                                                      'apcorr40','apcorr50','apcorr60','apcorr70',\n",
    "                                                                      'apcorr80','apcorr85','apcorr90','sky_in',\n",
    "                                                                      'sky_out'], comment='#',\n",
    "                        skiprows=0, usecols = range(0,26))\n",
    "aper_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Aperture Photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aperture_phot(filt=filt):\n",
    "    radii = [aper_table.loc[filt]['r70']]\n",
    "\n",
    "    ees = '70'.split()\n",
    "    ee_radii = dict(zip(ees,radii))\n",
    "\n",
    "    positions = np.transpose((dict_aper[det][filt]['stars for ap phot matched']['xcentroid'], \n",
    "                              dict_aper[det][filt]['stars for ap phot matched']['ycentroid']))\n",
    "    \n",
    "    image = fits.open(dict_images_combined[det][filt]['images'][0])\n",
    "    data_sb = image[1].data\n",
    "    imh = image[1].header\n",
    "    data = data_sb / imh['PHOTMJSR']\n",
    "\n",
    "    \n",
    "    # sky from the aperture correction table:\n",
    "\n",
    "    sky = {\"sky_in\": aper_table.loc[filt]['r80'], \"sky_out\": aper_table.loc[filt]['r85']}\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    table_aper = Table()\n",
    "\n",
    "    for ee,radius in ee_radii.items():\n",
    "        print(\"Performing aperture photometry for radius equivalent to EE = {0}% for filter {1}\".format(ee,filt))\n",
    "        aperture = CircularAperture(positions, r = radius)\n",
    "        annulus_aperture = CircularAnnulus(positions, r_in = sky[\"sky_in\"], r_out = sky[\"sky_out\"])\n",
    "        annulus_mask = annulus_aperture.to_mask(method = 'center')\n",
    "    \n",
    "        bkg_median = []\n",
    "        for mask in annulus_mask:\n",
    "            annulus_data = mask.multiply(data)\n",
    "            annulus_data_1d = annulus_data[mask.data > 0]\n",
    "            _,median_sigclip,_ = sigma_clipped_stats(annulus_data_1d)\n",
    "            bkg_median.append(median_sigclip)\n",
    "        bkg_median = np.array(bkg_median)\n",
    "    \n",
    "        phot = aperture_photometry(data, aperture, method = 'exact')\n",
    "        phot['annulus_median'] = bkg_median\n",
    "        phot['aper_bkg'] = bkg_median * aperture.area\n",
    "        phot['aper_sum_bkgsub'] = phot['aperture_sum'] - phot['aper_bkg']\n",
    "        \n",
    "        apcorr = [aper_table.loc[filt]['apcorr70']]\n",
    "\n",
    "        phot['aper_sum_corrected'] = phot['aper_sum_bkgsub'] * apcorr\n",
    "\n",
    "        phot['mag_corrected'] = -2.5 * np.log10(phot['aper_sum_corrected']) + dict_utils[filt]['VegaMAG zp modB']\n",
    "    \n",
    "        table_aper.add_column(phot['aperture_sum'], name = 'aper_sum_'+ee)\n",
    "        table_aper.add_column(phot['annulus_median'], name = 'annulus_median_'+ee)\n",
    "        table_aper.add_column(phot['aper_bkg'], name = 'aper_bkg_ee_'+ee)\n",
    "        table_aper.add_column(phot['aper_sum_bkgsub'], name = 'aper_sum_bkgsub_'+ee)\n",
    "        table_aper.add_column(phot['aper_sum_corrected'], name = 'aper_sum_corrected_'+filt) \n",
    "        table_aper.add_column(phot['mag_corrected'], name = 'mag_corrected_'+filt) \n",
    "        \n",
    "        \n",
    "        dict_aper[det][filt]['aperture phot table'] = table_aper\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(\"Time Elapsed:\", toc-tic)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aperture_phot(filt=filt1)\n",
    "aperture_phot(filt=filt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Zeropoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.clf()\n",
    "\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "\n",
    "ax1.set_xlabel(filt1+'_inst', fontdict = font2)\n",
    "ax1.set_ylabel(\"Zeropoint\", fontdict = font2)\n",
    "\n",
    "idx_zp_1, d2d_zp_1, _ = match_coordinates_sky(dict_aper[det][filt1]['stars for ap phot matched']['radec'], radec_f115w_inst)\n",
    "\n",
    "sep_constraint_zp_1 = d2d_zp_1 < max_sep\n",
    "\n",
    "f115w_ap_matched = np.array(dict_aper[det][filt1]['aperture phot table']['mag_corrected_'+filt1][sep_constraint_zp_1])\n",
    "f115w_psf_matched = np.array(df_f115w['f115w_inst'][idx_zp_1[sep_constraint_zp_1]])\n",
    "\n",
    "diff_f115w = f115w_ap_matched - f115w_psf_matched\n",
    "_,zp_f115w,zp_sigma_f115w = sigma_clipped_stats(diff_f115w)\n",
    "\n",
    "\n",
    "xlim0 = -9\n",
    "xlim1 = -5\n",
    "ylim0 = np.mean(diff_f115w) - 0.5 \n",
    "ylim1 = np.mean(diff_f115w) + 0.5\n",
    "\n",
    "ax1.set_xlim(xlim0,xlim1)\n",
    "ax1.set_ylim(ylim0,ylim1)\n",
    "\n",
    "ax1.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax1.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax1.scatter(f115w_psf_matched, diff_f115w, s = 50, color = 'k')\n",
    "ax1.plot([xlim0,xlim1],[zp_f115w,zp_f115w], color = 'r', lw = 5, ls = '--')\n",
    "ax1.text(xlim0+0.05, ylim1 - 0.15, filt1+' Zeropoint = %5.3f $\\pm$ %5.3f'%(zp_f115w, zp_sigma_f115w), color = 'k', fontdict=font2)\n",
    "                \n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax2.set_xlabel(filt2+'_inst', fontdict = font2)\n",
    "ax2.set_ylabel(\"Zeropoint\", fontdict = font2)\n",
    "\n",
    "idx_zp_2, d2d_zp_2, _ = match_coordinates_sky(dict_aper[det][filt2]['stars for ap phot matched']['radec'], radec_f200w_inst)\n",
    "\n",
    "sep_constraint_zp_2 = d2d_zp_2 < max_sep\n",
    "\n",
    "f200w_ap_matched = np.array(dict_aper[det][filt2]['aperture phot table']['mag_corrected_'+filt2][sep_constraint_zp_2])\n",
    "f200w_psf_matched = np.array(df_f200w['f200w_inst'][idx_zp_2[sep_constraint_zp_2]])\n",
    "\n",
    "diff_f200w = f200w_ap_matched - f200w_psf_matched\n",
    "_,zp_f200w,zp_sigma_f200w = sigma_clipped_stats(diff_f200w)\n",
    "\n",
    "\n",
    "xlim0 = -9\n",
    "xlim1 = -5\n",
    "ylim0 = np.mean(diff_f200w) - 0.5 \n",
    "ylim1 = np.mean(diff_f200w) + 0.5\n",
    "\n",
    "ax2.set_xlim(xlim0,xlim1)\n",
    "ax2.set_ylim(ylim0,ylim1)\n",
    "\n",
    "ax2.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax2.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax2.scatter(f200w_psf_matched, diff_f200w, s = 50, color = 'k')\n",
    "ax2.plot([xlim0,xlim1],[zp_f200w,zp_f200w], color = 'r', lw = 5, ls = '--')\n",
    "ax2.text(xlim0+0.05, ylim1 - 0.15, filt2+' Zeropoint = %5.3f $\\pm$ %5.3f'%(zp_f200w, zp_sigma_f200w), color = 'k', fontdict=font2)\n",
    "                \n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import input photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cat = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/stellar_photometry/pointsource.cat'\n",
    "\n",
    "cat = pd.read_csv(input_cat, header = None, sep='\\s+', names=['ra_in','dec_in','f070w_in','f115w_in',\n",
    "                                                                     'f200w_in', 'f277w_in', 'f356w_in', 'f444w_in'], \n",
    "                  comment='#', skiprows=7, usecols= range(0,8))\n",
    "\n",
    "cat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract from the input catalog the stars in the analyzed region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_ra_min = np.min(radec_f115w.ra)\n",
    "lim_ra_max = np.max(radec_f115w.ra)\n",
    "lim_dec_min = np.min(radec_f115w.dec)\n",
    "lim_dec_max = np.max(radec_f115w.dec)\n",
    "\n",
    "cat_sel = cat[(cat['ra_in'] > lim_ra_min) & (cat['ra_in'] < lim_ra_max) & (cat['dec_in'] > lim_dec_min)\n",
    "              & (cat['dec_in'] < lim_dec_max)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrated Color-Magnitude Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,14))\n",
    "plt.clf()\n",
    "\n",
    "font1 = {'family' : 'serif',\n",
    "      'color': 'black',\n",
    "      'weight': 'normal',\n",
    "      'size': '12'}\n",
    "\n",
    "font2 = {'family' : 'serif',\n",
    "      'color': 'black',\n",
    "      'weight': 'normal',\n",
    "      'size': '30'}\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "\n",
    "mag1_in = np.array(cat_sel['f115w_in'])\n",
    "mag2_in = np.array(cat_sel['f200w_in'])\n",
    "diff_in = mag1_in - mag2_in\n",
    "\n",
    "xlim0 = -0.25 \n",
    "xlim1 = 1.75 \n",
    "ylim0 = 25 \n",
    "ylim1 = 15 \n",
    "ax1.set_xlim(xlim0,xlim1)\n",
    "ax1.set_ylim(ylim0,ylim1)\n",
    "\n",
    "ax1.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax1.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax1.scatter(mag1_in - mag2_in, mag1_in, s = 1, color = 'k')\n",
    "\n",
    "ax1.set_xlabel(filt1+' - '+filt2, fontdict = font2)\n",
    "ax1.set_ylabel(filt1, fontdict = font2)\n",
    "ax1.text(xlim0+0.15, 15.5, \"Input\", fontdict = font2)\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "\n",
    "ax2.set_xlim(xlim0,xlim1)\n",
    "ax2.set_ylim(ylim0,ylim1)\n",
    "\n",
    "ax2.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax2.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "f115w = f115w_inst + zp_f115w \n",
    "f200w = f200w_inst + zp_f200w\n",
    "\n",
    "maglim = np.arange(18,25,1)\n",
    "mags = []\n",
    "errs_mag = []\n",
    "errs_col = []\n",
    "\n",
    "for i in np.arange(0,len(maglim)-1,1):\n",
    "    mag = (maglim[i] + maglim[i+1])/2\n",
    "    err_mag1 = ef115w_inst[(f115w > maglim[i]) & (f115w < maglim[i+1])]\n",
    "    err_mag2 = ef200w_inst[(f115w > maglim[i]) & (f115w < maglim[i+1])]\n",
    "    err_mag = np.mean(err_mag1[i])\n",
    "    err_temp = np.sqrt(err_mag1**2 + err_mag2**2)\n",
    "    err_col = np.mean(err_temp[i])\n",
    "    \n",
    "    errs_mag.append(err_mag)                  \n",
    "    errs_col.append(err_col)\n",
    "    mags.append(mag)\n",
    "\n",
    "col = [0] * (len(maglim)-1)\n",
    "\n",
    "ax2.errorbar(col, mags, yerr=errs_mag, xerr=errs_col, fmt='o', color = 'k')\n",
    "    \n",
    "    \n",
    "ax2.scatter(f115w - f200w, f115w, s = 1, color = 'k')\n",
    "ax2.text(xlim0+0.15, 15.5, \"Output\", fontdict = font2)\n",
    "\n",
    "ax2.set_xlabel(filt1+' - '+filt2, fontdict = font2)\n",
    "ax2.set_ylabel(filt1, fontdict = font2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between input and output photometry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.clf()\n",
    "\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "\n",
    "ax1.set_xlabel(filt1, fontdict = font2)\n",
    "ax1.set_ylabel('$\\Delta$ Mag', fontdict = font2)\n",
    "\n",
    "radec_input = SkyCoord(cat_sel['ra_in'],cat_sel['dec_in'], unit = 'deg')\n",
    "\n",
    "idx_f115w_cfr, d2d_f115w_cfr, _ = match_coordinates_sky(radec_input,radec_f115w)\n",
    "\n",
    "sep_f115w_cfr = d2d_f115w_cfr < max_sep\n",
    "\n",
    "f115w_inp_cfr = np.array(cat_sel['f115w_in'][sep_f115w_cfr])\n",
    "f115w_psf_cfr = np.array(f115w[idx_f115w_cfr[sep_f115w_cfr]])\n",
    "\n",
    "diff_f115w_cfr = f115w_inp_cfr - f115w_psf_cfr\n",
    "_,med_diff_f115w_cfr,sig_diff_f115w_cfr = sigma_clipped_stats(diff_f115w_cfr)\n",
    "\n",
    "xlim0 = 16\n",
    "xlim1 = 24\n",
    "ylim0 = np.mean(diff_f115w_cfr) - 0.5 \n",
    "ylim1 = np.mean(diff_f115w_cfr) + 0.5\n",
    "\n",
    "ax1.set_xlim(xlim0,xlim1)\n",
    "ax1.set_ylim(ylim0,ylim1)\n",
    "\n",
    "ax1.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax1.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax1.scatter(f115w_psf_cfr, diff_f115w_cfr, s = 5, color = 'k')\n",
    "ax1.plot([xlim0,xlim1],[0,0], color = 'r', lw = 5, ls = '--')\n",
    "ax1.text(xlim0+0.05, ylim1 - 0.15, filt1+' $\\Delta$ Mag = %5.3f $\\pm$ %5.3f'\n",
    "         %(med_diff_f115w_cfr, sig_diff_f115w_cfr), color = 'k', fontdict=font2)\n",
    "\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "\n",
    "ax2.set_xlabel(filt2, fontdict = font2)\n",
    "ax2.set_ylabel('$\\Delta$ Mag', fontdict = font2)\n",
    "\n",
    "idx_f200w_cfr, d2d_f200w_cfr, _ = match_coordinates_sky(radec_input,radec_f200w)\n",
    "\n",
    "sep_f200w_cfr = d2d_f200w_cfr < max_sep\n",
    "\n",
    "f200w_inp_cfr = np.array(cat_sel['f200w_in'][sep_f200w_cfr])\n",
    "f200w_psf_cfr = np.array(f200w[idx_f200w_cfr[sep_f200w_cfr]])\n",
    "\n",
    "\n",
    "diff_f200w_cfr = f200w_inp_cfr - f200w_psf_cfr\n",
    "_,med_diff_f200w_cfr,sig_diff_f200w_cfr = sigma_clipped_stats(diff_f200w_cfr)\n",
    "\n",
    "xlim0 = 16\n",
    "xlim1 = 24\n",
    "ylim0 = np.mean(diff_f200w_cfr) - 0.5 \n",
    "ylim1 = np.mean(diff_f200w_cfr) + 0.5\n",
    "\n",
    "ax2.set_xlim(xlim0,xlim1)\n",
    "ax2.set_ylim(ylim0,ylim1)\n",
    "\n",
    "ax2.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax2.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax2.scatter(f200w_psf_cfr, diff_f200w_cfr, s = 5, color = 'k')\n",
    "ax2.plot([xlim0,xlim1],[0,0], color = 'r', lw = 5, ls = '--')\n",
    "ax2.text(xlim0+0.05, ylim1 - 0.15, filt2+' $\\Delta$ Mag = %5.3f $\\pm$ %5.3f'%(med_diff_f200w_cfr, sig_diff_f200w_cfr), color = 'k', fontdict=font2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
